
- remove all global state:
    - Config object to represent the config
    - make_app to make the flask app
    - make_cache to make the schema and cache

- split non-indempotent parts of Api3ReadOperation into Api3ReadQuery

- return accurate paging info; currently we are cheating a bit

- redirect Shotgun pages to actual Shotgun

- devops
    - split up the processes
    - nginx proxy to real thing when down?

- how to perform rolling schema upgrades?
    1. move event watcher to new schema
    2. perform full scan
    3. restart interval scanner
    4. restart cache with new schema

    - this generally needs services to be independant
    - would also be great if we have a system for sending messages into the
      running services, and having them reload the schema, or adding fields
      live on the fly
        - unix socket in a thread with json messages
    
- get and parse private schema
    s = requests.Session()
    s.cookies['_session_id'] = sg._get_session_token()
    s.get('https://keystone.shotgunstudio.com/page/schema')



- use sudo_as_login in logs

- return a fake "name" (as determined by the identifier_column in the 
  private schema) with entities/multi-entities



- sgcache-scanner -> grab updated data periodically

- sgcache-reprocess --retirement -> reprocess retirement events

- table with log of data updates
- the actual event logs?




- docs:
    ./bin/grep-schema -f schema/basic-filters.txt schema/keystone-full.yaml > schema/keystone-basic.yml
    dev ./bin/dump-data -p 66 -s schema/keystone-basic.yml | tee data.json
    PYTHONPATH=.:$PYTHONPATH bin/update-data -u sqlite:///test.db -s schema/keystone-basic.yml data.json
    SGCACHE_SQLA_URL=sqlite:///test.db PYTHONPATH="$PYTHONPATH:." python -m sgcache.web


    ./bin/dump-data -p 115 -s schema/keystone-basic.yml | tee all.json
    PYTHONPATH=.:$PYTHONPATH bin/update-data -u sqlite:///test.db -s schema/keystone-basic.yml all.json


    SGCACHE_SQLA_URL=sqlite:///test.db PYTHONPATH="$PYTHONPATH:." python -m sgcache.web
    SGCACHE_SQLA_URL=postgres:///sgcache python -m sgcache.web

    ./bin/dump-data -p 66 -s schema/keystone-basic.yml | tee all.json
    ./bin/dump-data -p 115 -s schema/keystone-basic.yml | tee all.json
    PYTHONPATH=.:$PYTHONPATH ./bin/update-data -u postgres:///sgcache -s schema/keystone-basic.yml all.json



- dump-schema.py which creates a schema.json
- dump-data.py which takes a schema.json, and a $TYPE.json (which is an
  entity per line, json encoded)
    - sphinx docs for everything that we learn as we test it
    âˆš ask Shotgun how much it costs to host our own

- daemons:
    - master server which holds the last_event_id lock
    - tiny web app
    - one sgevent server (which could be another project)
    - multiple API servers (because Python threads suck)

- what must be do for `s3_uploads_enabled` in server info?

- how to pull apart the results of those queries

- URLS could include the real shotgun server:
    >>> sg = Shotgun('http://localhost:8000/next=keystone.shotgunstudio.com/', 'xxx', 'yyy')
    The trailing slash is required for it to not get parsed out.

- Can we instead exist as the http_proxy kwarg?

- listen for schema changes, and use that to invalidate data
