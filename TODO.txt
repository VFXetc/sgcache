
Task.content
  SELECT task.content from task WHERE ...

Task.entity
  SELECT task.entity__type, task.entity__id from task WHERE ...

Task.entity.Shot.name
  SELECT shot.name from task OUTER JOIN shot WHERE
    task.entity__type = 'Shot' AND task.entity__id = shot.id AND ...

IN GENERAL:
  fields in either return fields OR filters must be broken down into:
    1. required joins
    2. the final column(s), valid in the context of those joins

API:

  Field.prepare_select(req, path_to_here):
    - setup the joins (via the request)
    - add the columns to the request (via the request)
    - return some state that will be given back to extract_select

  Field.extract_select(req, row, state):
    - given the state, return a dict of the fields to return for the given row

  Field.prepare_filter(req, path_to_here, operator, values):
    - setup the joins (via the request)
    - return the SQLA statement to add to the filter

  Field.prepare_order(req, path_to_here):
    - setup the joins
    - return the SQLA statement to order by





  for path:
      joins: task JOIN shot ON task.entity__id = shot.id
  for field:
      joins: shot JOIN sequence on shot.sequence__id = sequence_id


- ReadQuery

  
  - table_for_key('entity.Shot.sequence') -> alias of Sequence table
    - this is unique for 'entity.Shot'; 'entity.Shot.something_else' gives the
      same table

  - column_for_key('entity.Shot.something') -> "something" from alias of Shot table

  - fields_to_select: the fields (which are columns from SQLA tables) to select
  
  - root_table: the table everything starts from
  - select_from: the query to select from eventually. This starts equal to the
    root table, and is expanded slowly
  - select_from_keys: a set of (table_alias.name, str(expr)), used by outer_join
    to only do the join once
  - outer_join(table, on_expr)



  - parses the filters and return fields, slowly building up
    the tables to join, columns to return, and names to refer to them by (so
    that we can have assets joined to assets).
  - The various fields are asked to
    prepare their parts, and they are given the names that they represent in
    that query (so that the aliases in the query can be sensical, e.g.:
    entity_Task_step_Step_code)
  - the fields also give a callback which is given each database row and the
    final entity, and must copy from one to the other


sg.find('Task', [('entity.Shot.id', 'is', 123)], ['entity.Shot.name'])

  SELECT task.id, shot.id, shot.name
  FROM
    tasks
      OUTER JOIN shots ON shot.id = task.entity__id
  WHERE
    shot.id = 123



- first steps:
    - dump-schema.py which creates a schema.json
    - dump-data.py which takes a schema.json, and a $TYPE.json (which is an
      entity per line, json encoded)
    - sphinx docs for everything that we learn as we test it
    âˆš ask Shotgun how much it costs to host our own

- daemons:
    - master server which holds the last_event_id lock
    - tiny web app
    - one sgevent server (which could be another project)
    - multiple API servers (because Python threads suck)

- how to build monstrous Neo4j queries to represent the common logic we want
  - define all relationships in the MATCH:
      MATCH (p:Project {id: 66}),(t:Task),t-[e:project]->p RETURN p, e, t LIMIT 1
      MATCH (e:Task), (e_project:Project {id: 66}), e-[:project]->e_project RETURN e, e_project LIMIT 1

      MATCH (e:Task), (e_project:Project {id: 66}), e-[:project]->e_project
          OPTIONAL MATCH (e_entity)<-[:entity]-(e)
          OPTIONAL MATCH (e_step)<-[:step]-(e)
          RETURN e, e_entity, e_step

      MATCH
          (t:Task),
          (h:Shot),
          (q:Sequence),
          (p:Project {id: 66}),
          (u:HumanUser {name: "Mike Boers"}),
          t-[:task_assignees]->u,
          t-[:project]->p,
          t-[:entity]->h,
          h-[:sg_sequence]->q
        RETURN t, p, u, h, q
        LIMIT 10

      MATCH (task:Task), (shot:Shot), task-[:entity]->shot
      RETURN task, shot as task_entity
      UNION ALL
      MATCH (task:Task), (asset:Asset), task-[:entity]->asset
      RETURN task, asset as task_entity
      LIMIT 10

- what must be do for `s3_uploads_enabled` in server info?

- store links as `!!entity Type id` so that we know that they exists

- how to pull apart the results of those queries

- YAML-style encoding of non-native types, e.g. a datetime: !!timestamp "2015-06-22 15:46:09.625815"

- URLS could include the real shotgun server:
    >>> sg = Shotgun('http://localhost:8000/next=keystone.shotgunstudio.com/', 'xxx', 'yyy')
    The trailing slash is required for it to not get parsed out.

- Can we instead exist as the http_proxy kwarg?

- framework for forwarding the query to the real Shotgun if we don't know
  how to deal with it

- listen for schema changes, and use that to invalidate data
