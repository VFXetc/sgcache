
- create sgevents which works around the event-log's problems:

    - group everything into transactions, and make further requests to make
      sure we have all events in a transaction

    - use $FROM$ to load retired entities in another request
        Event.entity -> entity field
        Event.retired_entity -> load via $FROM$

    - out-of-order events due to transactions
        - keep a pool of event IDS that we haven't seen yet, that we should
          have
        - only hold onto this pool for a limited time; perhaps 60s

- log query times
- update updated_at and updated_by with info from the event log

- page_count and entity_count are supposed to be for the read request
  without paging applied (but with the current filters)

- deal with retirement:
    - delete entities as soon as they are retired?
        - this could make dealing with a backlog of events tricky, as we would
          have to not revive entities by accident
    - s/_active/retirement_date ??
        - it seems like this could be reasonable attribute to use. if it is
          null then it is retired
    - check _active
        - check on every select and every join

- table with log of data updates
- the actual event logs?




- docs:
    ./bin/grep-schema -f schema/basic-filters.txt schema/keystone-full.yaml > schema/keystone-basic.yml
    dev ./bin/dump-data -p 66 -s schema/keystone-basic.yml | tee data.json
    PYTHONPATH=.:$PYTHONPATH bin/update-data -u sqlite:///test.db -s schema/keystone-basic.yml data.json
    SGCACHE_SQLA_URL=sqlite:///test.db PYTHONPATH="$PYTHONPATH:." python -m sgcache.web


    ./bin/dump-data -p 115 -s schema/keystone-basic.yml | tee all.json
    PYTHONPATH=.:$PYTHONPATH bin/update-data -u sqlite:///test.db -s schema/keystone-basic.yml all.json


    SGCACHE_SQLA_URL=sqlite:///test.db PYTHONPATH="$PYTHONPATH:." python -m sgcache.web
    SGCACHE_SQLA_URL=postgres:///sgcache python -m sgcache.web

    ./bin/dump-data -p 66 -s schema/keystone-basic.yml | tee all.json
    ./bin/dump-data -p 115 -s schema/keystone-basic.yml | tee all.json
    PYTHONPATH=.:$PYTHONPATH ./bin/update-data -u postgres:///sgcache -s schema/keystone-basic.yml all.json



- dump-schema.py which creates a schema.json
- dump-data.py which takes a schema.json, and a $TYPE.json (which is an
  entity per line, json encoded)
    - sphinx docs for everything that we learn as we test it
    âˆš ask Shotgun how much it costs to host our own

- daemons:
    - master server which holds the last_event_id lock
    - tiny web app
    - one sgevent server (which could be another project)
    - multiple API servers (because Python threads suck)

- what must be do for `s3_uploads_enabled` in server info?

- how to pull apart the results of those queries

- YAML-style encoding of non-native types, e.g. a datetime: !!timestamp "2015-06-22 15:46:09.625815"

- URLS could include the real shotgun server:
    >>> sg = Shotgun('http://localhost:8000/next=keystone.shotgunstudio.com/', 'xxx', 'yyy')
    The trailing slash is required for it to not get parsed out.

- Can we instead exist as the http_proxy kwarg?

- framework for forwarding the query to the real Shotgun if we don't know
  how to deal with it

- listen for schema changes, and use that to invalidate data
