
- unit test all the things!
    - direct through the cache
    - via events
    - via the scanner

- have a DATA config that will contain whatever runtime data we want, e.g.
  sockets, queues, etc.. Config.get_path(name, **kwargs) joins onto DATA after
  formatting with **kwargs

- cache.send_control_message(context_name, msg_name, **kwargs, wait=False)
    - when TESTING, respond to a `control` api call, to send a control message
    - wait -> True implies that it should wait for a response to send back

- Join through multi_entity fields.

    - Need to build a subquery, and run it in an EXISTS:
        q = select([parent.c.id]).where(
            exists().where(parent.c.id == child.c.pid)
        )

    - Refactor fields.py into a package of field types

    - Refactor Api3ReadOperation into a SelectBuilder and the Api3ReadOperation.
        - SelectBuilder has (from Api3ReadOperation)
            - entity_type
            - aliases
            - select_fields
            - select_from
            - joined
            - where_clauses
            - group_by_clauses
            - order_by_clauses
        - assert all Field methods work with the SelectBuilder


- Document the ping/pong feature, OR remove it after adding {'sgcache': true} to
  the info call.

- Restrict results of multi_entity to unique results. Shotgun does not allow
  copies of the same entity, and it does not respect the order.

- Contain the schema completely within the database itself. Column fields include:
    - db_name: The database column name.
    - sg_name: The shotgun field name.
    - do_cache: it should be cached by the events and scanner
    - do_expose: it should be considered by the web API

    With this, it gets TONS easier to make safe schema migrations.



OLDER (and not nessesarily still TODD)
======================================


- If we cache all entity and multi_entity fields, then we can infer the
  existance of new entities upon return of a creation. E.g. creating a Shot
  with a task_template results in a few new Tasks. If we cache Shot.tasks,
  then we will immediately see those, and the multi_entity field can be
  responsible for making sure they exist. Likely a _complete column would be
  added to signal incomplete entities which need fetching. A thread can wait
  for a signal that such entities exist, then scan the database looking for them,
  and fully cache them.

- We can cache SOME url fields, depending on their "link_type" (e.g. we can
  cache those of "web" type). Would generally need support for raising an
  exception for a single field of a single entity.

  If we want to immediately deal with partially cached data, start by collecting
  the uncached_entities and uncached_fields on some object. Those must then
  be resolved immediatly via a ('id', 'in', set_of_ids) query (because if
  we do a simple passthrough that requires our order_by to match that of
  Shotgun's).

  Another method is to have another thread listening to a queue for encounters
  with uncached values. When encountered, a signal can be sent to that thread,
  and an exception raised in order to pass the request through.

  Passthrough find(s) could be augmented to return every field, such that we
  are able to cache those entities immediately.

- multi_entity is_not None -> True if there is something in there

- Skip change events that follow a create event.

- split non-indempotent parts of Api3ReadOperation into Api3ReadQuery

- return accurate paging info; currently we are cheating a bit

- how to perform rolling schema upgrades?
    1. move event watcher to new schema
    2. perform full scan
    3. restart interval scanner
    4. restart cache with new schema

    - this generally needs services to be independant
    - would also be great if we have a system for sending messages into the
      running services, and having them reload the schema, or adding fields
      live on the fly
        - unix socket in a thread with json messages

- get and parse private schema
    s = requests.Session()
    s.cookies['_session_id'] = sg._get_session_token()
    s.get('https://keystone.shotgunstudio.com/page/schema')

- return a fake "name" (as determined by the identifier_column in the
  private schema) with entities/multi-entities

- sgcache-reprocess --retirement -> reprocess retirement events

- table with log of data updates
- the actual event logs?

- docs:
    ./bin/grep-schema -f schema/basic-filters.txt schema/keystone-full.yaml > schema/keystone-basic.yml
    dev ./bin/dump-data -p 66 -s schema/keystone-basic.yml | tee data.json
    PYTHONPATH=.:$PYTHONPATH bin/update-data -u sqlite:///test.db -s schema/keystone-basic.yml data.json
    SGCACHE_SQLA_URL=sqlite:///test.db PYTHONPATH="$PYTHONPATH:." python -m sgcache.web


    ./bin/dump-data -p 115 -s schema/keystone-basic.yml | tee all.json
    PYTHONPATH=.:$PYTHONPATH bin/update-data -u sqlite:///test.db -s schema/keystone-basic.yml all.json


    SGCACHE_SQLA_URL=sqlite:///test.db PYTHONPATH="$PYTHONPATH:." python -m sgcache.web
    SGCACHE_SQLA_URL=postgres:///sgcache python -m sgcache.web

    ./bin/dump-data -p 66 -s schema/keystone-basic.yml | tee all.json
    ./bin/dump-data -p 115 -s schema/keystone-basic.yml | tee all.json
    PYTHONPATH=.:$PYTHONPATH ./bin/update-data -u postgres:///sgcache -s schema/keystone-basic.yml all.json

- what must be do for `s3_uploads_enabled` in server info?

- URLS could include the real shotgun server:
    >>> sg = Shotgun('http://localhost:8000/next=keystone.shotgunstudio.com/', 'xxx', 'yyy')
    The trailing slash is required for it to not get parsed out.

- Can we instead exist as the http_proxy kwarg?

- listen for schema changes, and use that to invalidate data
